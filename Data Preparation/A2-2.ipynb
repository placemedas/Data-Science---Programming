{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2: Entity Resolution (Part 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Assignment 2 (Part 2), you will learn how to use Active Learning to address the entity resolution problem. After completing this assignment, you should be able to answer the following questions:\n",
    "\n",
    "1. Why Active Learning?\n",
    "2. How to implement uncertain sampling, a popular query strategy for Active Learning?\n",
    "3. How to solve an ER problem using Active Learning?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Active Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Active learning](http://tiny.cc/al-wiki) is a certain type of ML algorithms that can train a high-quality ML model with small data-labeling cost. Its basic idea is quite easy to understand. Consider a typical supervised ML problem, which requires a (relatively) large training dataset. In the training dataset, there may be only a small number of data points that are beneficial to the trained ML model. In other words, labeling a small number of data points is enough to train a high-quality ML model. The goal of active learning is to help us to identify those data points. \n",
    "\n",
    "\n",
    "In this assignment, we will develop an Active Learning approach for Entity Resolution. The following figure shows the architecture of an entity resolution solution. It consists of four major steps. **I will provide you the source code for Steps 1, 2, 4. Your job is to implement Step 3.**  \n",
    "\n",
    "<img src=\"img/arch.png\", width=800/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1. Read Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we get a restaurant dataset `restaurant.csv`. The data has many duplicate restaurants.  For example, the first two rows shown below are duplicated (i.e., refer to the same real-world entity). You can check out all duplicate (matching) record pairs from `true_matches.json`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(#Rows, #Cols) : (858, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>address</th>\n",
       "      <th>city</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>arnie morton's of chicago</td>\n",
       "      <td>435 s. la cienega blv.</td>\n",
       "      <td>los angeles</td>\n",
       "      <td>american</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>arnie morton's of chicago</td>\n",
       "      <td>435 s. la cienega blvd.</td>\n",
       "      <td>los angeles</td>\n",
       "      <td>steakhouses</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>art's delicatessen</td>\n",
       "      <td>12224 ventura blvd.</td>\n",
       "      <td>studio city</td>\n",
       "      <td>american</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>art's deli</td>\n",
       "      <td>12224 ventura blvd.</td>\n",
       "      <td>studio city</td>\n",
       "      <td>delis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>hotel bel-air</td>\n",
       "      <td>701 stone canyon rd.</td>\n",
       "      <td>bel air</td>\n",
       "      <td>californian</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                       name                  address         city  \\\n",
       "0   1  arnie morton's of chicago   435 s. la cienega blv.  los angeles   \n",
       "1   2  arnie morton's of chicago  435 s. la cienega blvd.  los angeles   \n",
       "2   3         art's delicatessen      12224 ventura blvd.  studio city   \n",
       "3   4                 art's deli      12224 ventura blvd.  studio city   \n",
       "4   5              hotel bel-air     701 stone canyon rd.      bel air   \n",
       "\n",
       "          type  \n",
       "0     american  \n",
       "1  steakhouses  \n",
       "2     american  \n",
       "3        delis  \n",
       "4  californian  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('restaurant.csv')\n",
    "data = df.values.tolist()\n",
    "print(\"(#Rows, #Cols) :\", df.shape)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2. Similar Pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first use a similarity-join algorithm to generate similar pairs. \n",
    "\n",
    "Below is the code. After running the code, we get 678 similar pairs ordered by their similarity decreasingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of Pairs:  367653.0\n",
      "Num of Similar Pairs:  678\n",
      "The Most Similar Pair:  ([170, \"mary mac's tea room\", '224 ponce de leon ave.', 'atlanta', 'southern/soul'], [169, \"mary mac's tea room\", '224 ponce de leon ave.', 'atlanta', 'southern'])\n"
     ]
    }
   ],
   "source": [
    "from a2_utils import *\n",
    "\n",
    "data = df.values.tolist()\n",
    "\n",
    "simpairs = simjoin(data)\n",
    "\n",
    "print(\"Num of Pairs: \", len(data)*(len(data)-1)/2)\n",
    "print(\"Num of Similar Pairs: \", len(simpairs))\n",
    "print(\"The Most Similar Pair: \", simpairs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that `simjoin` helps us remove the number of pairs from 367653 to 678. But, there are still many non-matching pairs in `simpairs` (see below). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([764, \"buzio's in the rio\", '3700 w. flamingo rd.', 'las vegas', 'seafood'], [542, 'carnival world', '3700 w. flamingo rd.', 'las vegas', 'buffets'])\n"
     ]
    }
   ],
   "source": [
    "print(simpairs[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will use active learning to train a classifier, and then use the classifier to classify each pair in `simpairs` as either \"matching\" or \"nonmatching\". "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3. Active Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a set of similar pairs, what you need to do next is to iteratively train a classifier to decide which pairs are truly matching. We are going to use [logistic regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) as our classifier. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the beginning, all the pairs are unlabeled. To initialize a model, we first pick up ten pairs and then label each pair using  the `crowdsourcing()` function. You can assume that `crowdsourcing()` will ask a crowd worker (e.g., on Amazon Mechanical Turk) to label a pair. \n",
    "\n",
    "\n",
    "`crowdsourcing(pair)` is a function that simulates the use of crowdsourcing to label a pair\n",
    "  \n",
    "  - **Input:**\tpair – A pair of records \n",
    "\n",
    "  - **Output:**\tBoolean –  *True*: The pair of records are matching; *False*: The pair of records are NOT matching;\n",
    "\n",
    "Please use the following code to do the initialization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31mAre they matching?\u001b[0m\n",
      "[170, \"mary mac's tea room\", '224 ponce de leon ave.', 'atlanta', 'southern/soul']\n",
      "[169, \"mary mac's tea room\", '224 ponce de leon ave.', 'atlanta', 'southern']\n",
      "\u001b[1;31mAnswer: Yes\u001b[0m\n",
      "\u001b[1;31mAre they matching?\u001b[0m\n",
      "[88, 'manhattan ocean club', '57 w. 58th st.', 'new york city', 'seafood']\n",
      "[87, 'manhattan ocean club', '57 w. 58th st.', 'new york', 'seafood']\n",
      "\u001b[1;31mAnswer: Yes\u001b[0m\n",
      "\u001b[1;31mAre they matching?\u001b[0m\n",
      "[112, 'san domenico', '240 central park s.', 'new york city', 'italian']\n",
      "[111, 'san domenico', '240 central park s', 'new york', 'italian']\n",
      "\u001b[1;31mAnswer: Yes\u001b[0m\n",
      "\u001b[1;31mAre they matching?\u001b[0m\n",
      "[197, 'fleur de lys', '777 sutter st.', 'san francisco', 'french (new)']\n",
      "[196, 'fleur de lys', '777 sutter st.', 'san francisco', 'french']\n",
      "\u001b[1;31mAnswer: Yes\u001b[0m\n",
      "\u001b[1;31mAre they matching?\u001b[0m\n",
      "[8, 'cafe bizou', '14016 ventura blvd.', 'sherman oaks', 'french bistro']\n",
      "[7, 'cafe bizou', '14016 ventura blvd.', 'sherman oaks', 'french']\n",
      "\u001b[1;31mAnswer: Yes\u001b[0m\n",
      "\u001b[1;31mAre they matching?\u001b[0m\n",
      "[709, 'arcadia', '21 e. 62nd st.', 'new york city', 'american (new)']\n",
      "[66, 'four seasons', '99 e. 52nd st.', 'new york city', 'american (new)']\n",
      "\u001b[1;31mAnswer: No\u001b[0m\n",
      "\u001b[1;31mAre they matching?\u001b[0m\n",
      "[709, 'arcadia', '21 e. 62nd st.', 'new york city', 'american (new)']\n",
      "[70, 'gramercy tavern', '42 e. 20th st.', 'new york city', 'american (new)']\n",
      "\u001b[1;31mAnswer: No\u001b[0m\n",
      "\u001b[1;31mAre they matching?\u001b[0m\n",
      "[729, 'la grenouille', '3 e. 52nd st.', 'new york city', 'french (classic)']\n",
      "[60, 'daniel', '20 e. 76th st.', 'new york city', 'french (new)']\n",
      "\u001b[1;31mAnswer: No\u001b[0m\n",
      "\u001b[1;31mAre they matching?\u001b[0m\n",
      "[733, 'menchanko-tei', '39 w. 55th st.', 'new york city', 'japanese']\n",
      "[76, 'la caravelle', '33 w. 55th st.', 'new york city', 'french (classic)']\n",
      "\u001b[1;31mAnswer: No\u001b[0m\n",
      "\u001b[1;31mAre they matching?\u001b[0m\n",
      "[764, \"buzio's in the rio\", '3700 w. flamingo rd.', 'las vegas', 'seafood']\n",
      "[542, 'carnival world', '3700 w. flamingo rd.', 'las vegas', 'buffets']\n",
      "\u001b[1;31mAnswer: No\u001b[0m\n",
      "Number of matches:  5\n",
      "Number of nonmatches:  5\n"
     ]
    }
   ],
   "source": [
    "from a2_utils import crowdsourcing\n",
    "\n",
    "# choose the most/least similar five pairs as initial training data\n",
    "init_pairs = simpairs[:5] + simpairs[-5:]\n",
    "matches = [] \n",
    "label_m = []\n",
    "label_n = []\n",
    "nonmatches = []\n",
    "for pair in init_pairs:\n",
    "    is_match = crowdsourcing(pair)\n",
    "    if is_match == True:\n",
    "        matches.append(pair)\n",
    "        label_m.append(1)\n",
    "    else:\n",
    "        nonmatches.append(pair)\n",
    "        label_n.append(0)\n",
    "        \n",
    "print(\"Number of matches: \", len(matches))\n",
    "print(\"Number of nonmatches: \", len(nonmatches))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the only code you need to write in this assignment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from a2_utils import featurize, crowdsourcing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer,TfidfTransformer\n",
    "\n",
    "labeled_pairs = matches + nonmatches\n",
    "# Obtaining the labels for matching and non-matching pairs. Matches are 1 and non-matches are 0.\n",
    "label = label_m + label_n\n",
    "unlabeled_pairs = [p for p in simpairs if p not in labeled_pairs]\n",
    "iter_num = 5\n",
    "\n",
    "# Vectorizing the features\n",
    "labeled_features = np.array([featurize(lb) for lb in labeled_pairs])\n",
    "unlabeled_features = np.array([featurize(nlb) for nlb in unlabeled_pairs])\n",
    "    \n",
    "# Applying Logistic Regression and solver is liblinear since dataset is small and gives maxium F1 score. \n",
    "log = LogisticRegression(solver='liblinear')\n",
    "model = log.fit(labeled_features,label)\n",
    "\n",
    "#<-- Write Your Code -->\n",
    "\n",
    "for i in range(iter_num):\n",
    "\n",
    "    #Obtaining the probability of classes    \n",
    "    prob_arr = model.predict_proba(unlabeled_features)\n",
    "    \n",
    "    # Fetching the maximum probability for each predicted row and then take min from entire list\n",
    "    # to find the most uncertain pair\n",
    "    max_prob = np.max(prob_arr,axis=1)\n",
    "    indx = np.where(max_prob == np.min(max_prob))\n",
    "    \n",
    "    # Append the labelled pair list with most uncertain pair\n",
    "    labeled_pairs.append(unlabeled_pairs[indx[0][0]])\n",
    "    \n",
    "    #Obtain the label for most uncertain pair using crowd sourcing.Used crowdsourcing_fast to avoid delays\n",
    "    is_match = crowdsourcing_fast(unlabeled_pairs[indx[0][0]])\n",
    "    if is_match == True:\n",
    "        label.append(1)\n",
    "    else:\n",
    "        label.append(0)    \n",
    "    \n",
    "    # Delete the most uncertain pair from the unlabelled pair\n",
    "    unlabeled_pairs = np.delete(unlabeled_pairs, indx, 0)\n",
    "    \n",
    "    # Re-Vectorizing the features\n",
    "    labeled_features = np.array([featurize(lb) for lb in labeled_pairs])\n",
    "    unlabeled_features = np.array([featurize(nlb) for nlb in unlabeled_pairs])\n",
    "    \n",
    "    # Re-applying Logistic Regression and solver is liblinear since dataset is small and gives maxium F1 score. \n",
    "    log = LogisticRegression(solver='liblinear')\n",
    "    model = log.fit(labeled_features,label)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Algorithm Description].**   Active learning has many [query strategies](http://tiny.cc/al-wiki-qs) to decide which data point should be labeled. You need to implement uncertain sampling. The algorithm trains an initial model on `labeled_pairs`. Then, it iteratively trains a model. At each iteration, it first applies the model to `unlabeled_pairs`, and makes a prediction on each unlabeled pair along with a probability, where the probability indicates the confidence of the prediction. After that, it selects the most uncertain pair (If there is still a tie, break it randomly),  and call the `crowdsroucing()` function to label the pair. After the pair is labeled, it updates `labeled_pairs` and `unlabeled_pairs`, and then retrain the model on `labeled_pairs`.\n",
    "\n",
    "**[Input].** \n",
    "- `labeled_pairs`: 10 labeled pairs (by default)\n",
    "- `unlabeled_pairs`: 668 unlabeled pairs (by default)\n",
    "- `iter_num`: 5 (by default)\n",
    "\n",
    "**[Output].** \n",
    "- `model`: A logistic regression model built by scikit-learn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4. Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training an model, you can use the following code to evalute it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.8660714285714286\n",
      "Recall: 0.9150943396226415\n",
      "Fscore: 0.8899082568807338\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from a2_utils import evaluate\n",
    "\n",
    "            \n",
    "sp_features = np.array([featurize(sp) for sp in simpairs])\n",
    "label = model.predict(sp_features)\n",
    "pair_label = zip(simpairs, label)\n",
    "\n",
    "identified_matches = []\n",
    "for pair, label in pair_label:\n",
    "    if label == 1:\n",
    "        identified_matches.append(pair)\n",
    "        \n",
    "precision, recall, fscore = evaluate(identified_matches)\n",
    "\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"Fscore:\", fscore)\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete the code in A2-2.ipynb, and submit it to the CourSys activity Assignment 2."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
